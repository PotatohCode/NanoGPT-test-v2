{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124a869a",
   "metadata": {},
   "source": [
    "### Step 1: Install necesscary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b82f8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.7-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.3-cp312-cp312-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Using cached fonttools-4.60.1-cp312-cp312-win_amd64.whl.metadata (114 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.9-cp312-cp312-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\yeetmon\\documents\\github\\nanogpt-test-v2\\sc3000\\lib\\site-packages (from matplotlib) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\yeetmon\\documents\\github\\nanogpt-test-v2\\sc3000\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\yeetmon\\documents\\github\\nanogpt-test-v2\\sc3000\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Collecting pyparsing>=3 (from matplotlib)\n",
      "  Using cached pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\yeetmon\\documents\\github\\nanogpt-test-v2\\sc3000\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yeetmon\\documents\\github\\nanogpt-test-v2\\sc3000\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Using cached matplotlib-3.10.7-cp312-cp312-win_amd64.whl (8.1 MB)\n",
      "Using cached contourpy-1.3.3-cp312-cp312-win_amd64.whl (226 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.60.1-cp312-cp312-win_amd64.whl (2.3 MB)\n",
      "Using cached kiwisolver-1.4.9-cp312-cp312-win_amd64.whl (73 kB)\n",
      "Using cached pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.60.1 kiwisolver-1.4.9 matplotlib-3.10.7 pyparsing-3.2.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\yeetmon\\documents\\github\\nanogpt-test-v2\\sc3000\\lib\\site-packages (2.9.0+cu130)\n",
      "Requirement already satisfied: numpy in c:\\users\\yeetmon\\documents\\github\\nanogpt-test-v2\\sc3000\\lib\\site-packages (2.3.3)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.3.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting tiktoken\n",
      "  Using cached tiktoken-0.12.0-cp312-cp312-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting wandb\n",
      "  Using cached wandb-0.22.2-py3-none-win_amd64.whl.metadata (10 kB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\yeetmon\\documents\\github\\nanogpt-test-v2\\sc3000\\lib\\site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\yeetmon\\documents\\github\\nanogpt-test-v2\\sc3000\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\yeetmon\\documents\\github\\nanogpt-test-v2\\sc3000\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\yeetmon\\documents\\github\\nanogpt-test-v2\\sc3000\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\yeetmon\\documents\\github\\nanogpt-test-v2\\sc3000\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\yeetmon\\documents\\github\\nanogpt-test-v2\\sc3000\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\yeetmon\\documents\\github\\nanogpt-test-v2\\sc3000\\lib\\site-packages (from torch) (70.2.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\yeetmon\\documents\\github\\nanogpt-test-v2\\sc3000\\lib\\site-packages (from transformers) (25.0)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Using cached pyyaml-6.0.3-cp312-cp312-win_amd64.whl.metadata (2.4 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2025.10.23-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Using cached pyarrow-21.0.0-cp312-cp312-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Using cached pandas-2.3.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting httpx<1.0.0 (from datasets)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.6.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting click>=8.0.1 (from wandb)\n",
      "  Using cached click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Using cached gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\yeetmon\\documents\\github\\nanogpt-test-v2\\sc3000\\lib\\site-packages (from wandb) (4.5.0)\n",
      "Collecting protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 (from wandb)\n",
      "  Using cached protobuf-6.33.0-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Collecting pydantic<3 (from wandb)\n",
      "  Using cached pydantic-2.12.3-py3-none-any.whl.metadata (87 kB)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb)\n",
      "  Using cached sentry_sdk-2.42.1-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\yeetmon\\documents\\github\\nanogpt-test-v2\\sc3000\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohttp-3.13.1-cp312-cp312-win_amd64.whl.metadata (8.4 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Using cached gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting anyio (from httpx<1.0.0->datasets)\n",
      "  Using cached anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting certifi (from httpx<1.0.0->datasets)\n",
      "  Using cached certifi-2025.10.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx<1.0.0->datasets)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx<1.0.0->datasets)\n",
      "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1.0.0->datasets)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3->wandb)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.4 (from pydantic<3->wandb)\n",
      "  Using cached pydantic_core-2.41.4-cp312-cp312-win_amd64.whl.metadata (7.4 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3->wandb)\n",
      "  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->transformers)\n",
      "  Using cached charset_normalizer-3.4.4-cp312-cp312-win_amd64.whl.metadata (38 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\yeetmon\\documents\\github\\nanogpt-test-v2\\sc3000\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\yeetmon\\documents\\github\\nanogpt-test-v2\\sc3000\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\yeetmon\\documents\\github\\nanogpt-test-v2\\sc3000\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Using cached frozenlist-1.8.0-cp312-cp312-win_amd64.whl.metadata (21 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Using cached multidict-6.7.0-cp312-cp312-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Using cached propcache-0.4.1-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Using cached yarl-1.22.0-cp312-cp312-win_amd64.whl.metadata (77 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Using cached smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yeetmon\\documents\\github\\nanogpt-test-v2\\sc3000\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx<1.0.0->datasets)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Using cached transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "Downloading datasets-4.3.0-py3-none-any.whl (506 kB)\n",
      "Using cached tiktoken-0.12.0-cp312-cp312-win_amd64.whl (878 kB)\n",
      "Using cached wandb-0.22.2-py3-none-win_amd64.whl (19.1 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached click-8.3.0-py3-none-any.whl (107 kB)\n",
      "Using cached dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Using cached gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached protobuf-6.33.0-cp310-abi3-win_amd64.whl (436 kB)\n",
      "Using cached pyarrow-21.0.0-cp312-cp312-win_amd64.whl (26.2 MB)\n",
      "Using cached pydantic-2.12.3-py3-none-any.whl (462 kB)\n",
      "Using cached pydantic_core-2.41.4-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "Using cached pyyaml-6.0.3-cp312-cp312-win_amd64.whl (154 kB)\n",
      "Using cached regex-2025.10.23-cp312-cp312-win_amd64.whl (276 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached safetensors-0.6.2-cp38-abi3-win_amd64.whl (320 kB)\n",
      "Using cached sentry_sdk-2.42.1-py2.py3-none-any.whl (380 kB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "Using cached pandas-2.3.3-cp312-cp312-win_amd64.whl (11.0 MB)\n",
      "Using cached xxhash-3.6.0-cp312-cp312-win_amd64.whl (31 kB)\n",
      "Using cached aiohttp-3.13.1-cp312-cp312-win_amd64.whl (452 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached certifi-2025.10.5-py3-none-any.whl (163 kB)\n",
      "Using cached charset_normalizer-3.4.4-cp312-cp312-win_amd64.whl (107 kB)\n",
      "Using cached gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached anyio-4.11.0-py3-none-any.whl (109 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Using cached frozenlist-1.8.0-cp312-cp312-win_amd64.whl (44 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached multidict-6.7.0-cp312-cp312-win_amd64.whl (46 kB)\n",
      "Using cached propcache-0.4.1-cp312-cp312-win_amd64.whl (41 kB)\n",
      "Using cached smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached yarl-1.22.0-cp312-cp312-win_amd64.whl (87 kB)\n",
      "Installing collected packages: pytz, xxhash, urllib3, tzdata, typing-inspection, tqdm, sniffio, smmap, safetensors, regex, pyyaml, pydantic-core, pyarrow, protobuf, propcache, multidict, idna, h11, frozenlist, dill, click, charset_normalizer, certifi, attrs, annotated-types, aiohappyeyeballs, yarl, sentry-sdk, requests, pydantic, pandas, multiprocess, httpcore, gitdb, anyio, aiosignal, tiktoken, huggingface-hub, httpx, gitpython, aiohttp, wandb, tokenizers, transformers, datasets\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.1 aiosignal-1.4.0 annotated-types-0.7.0 anyio-4.11.0 attrs-25.4.0 certifi-2025.10.5 charset_normalizer-3.4.4 click-8.3.0 datasets-4.3.0 dill-0.4.0 frozenlist-1.8.0 gitdb-4.0.12 gitpython-3.1.45 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 huggingface-hub-0.36.0 idna-3.11 multidict-6.7.0 multiprocess-0.70.16 pandas-2.3.3 propcache-0.4.1 protobuf-6.33.0 pyarrow-21.0.0 pydantic-2.12.3 pydantic-core-2.41.4 pytz-2025.2 pyyaml-6.0.3 regex-2025.10.23 requests-2.32.5 safetensors-0.6.2 sentry-sdk-2.42.1 smmap-5.0.2 sniffio-1.3.1 tiktoken-0.12.0 tokenizers-0.22.1 tqdm-4.67.1 transformers-4.57.1 typing-inspection-0.4.2 tzdata-2025.2 urllib3-2.5.0 wandb-0.22.2 xxhash-3.6.0 yarl-1.22.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n",
    "!pip install torch numpy transformers datasets tiktoken wandb tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2d9de0",
   "metadata": {},
   "source": [
    "### Step 2: Package imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "876dd92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\")) \n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import pickle\n",
    "from model import GPT, GPTConfig\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configuration\n",
    "beta = 0.5 # controls DPO loss strength\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "base_lr = 1e-4 # learning rate\n",
    "epochs = 10 # number of runs of dataset\n",
    "batch_size = 128 # number of sample pairs processed together\n",
    "max_length =64 # max sequence length\n",
    "num_samples = 1\n",
    "max_new_tokens = 200 # max tokens generated\n",
    "temperature = 0.8 # randomness in generation\n",
    "top_k = 200 # sample from top_k most likely tokens\n",
    "# tokenizer\n",
    "\n",
    "with open(\"../sft/meta.pkl\", \"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
    "\n",
    "extra_chars = list(\"+-*/=xy?!,.' \")\n",
    "for ch in extra_chars:\n",
    "    if ch not in stoi:\n",
    "        new_index = len(stoi)\n",
    "        stoi[ch] = new_index\n",
    "        itos[new_index] = ch\n",
    "\n",
    "def encode(s): return [stoi[c] for c in s]\n",
    "def decode(l): return ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7d35e6",
   "metadata": {},
   "source": [
    "### Step 3: Define helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd97492",
   "metadata": {},
   "source": [
    "We wanted to check the CUDA installation on the computer before moving onto the next step. We also used this step to verify torch version compatibility with the CUDA version and the availability of GPU.\n",
    "\n",
    "On our device, the PyTorch version is 2.9.0 with CUDA 13.0 and 1 GPU device available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f36a492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0+cu130\n",
      "13.0\n",
      "1\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.is_available())\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db1983a",
   "metadata": {},
   "source": [
    "Calculating log probability, which is used over actual probability to prevent underflow, ensuring computational stability.\n",
    "Initially, the compute_logprob(input_ids) creates input/target pairs and uses the inputs to get the model predictions. \n",
    "Next, extract the batch size, sequence length time and vocab size from the logits, then reshapes the dimensions of the data to fit the model’s learning.\n",
    "The functions return negative loss after their calculations. Negative loss = log probability (Higher is better)\n",
    "Used for computing P(positive | question) and P(negative | question).\n",
    "\n",
    "pad_or_truncate(seq, max_length) makes all input sequences the same length when processing the inputs.\n",
    "\n",
    "get_batches(lines, batch_size) generates training batches from the given data.\n",
    "The function iterates through the data in chunks and process the negative & positive samples.\n",
    "Returns negative & positive batches after converting to tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d03655c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logprob(input_ids):\n",
    "    inputs = input_ids[:, :-1]\n",
    "    targets = input_ids[:, 1:]\n",
    "    logits, _ = gpt(inputs, full_seq=True)\n",
    "    B, T, V = logits.size()\n",
    "    logits_flat = logits.reshape(-1, V)\n",
    "    targets_flat = targets.reshape(-1)\n",
    "    loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=0, reduction='none')\n",
    "    loss = loss.reshape(B, T)\n",
    "    attention_mask = (targets != 0).float()\n",
    "    loss = (loss * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n",
    "    return -loss \n",
    "\n",
    "def pad_or_truncate(seq, max_length):\n",
    "    return seq[-max_length:] if len(seq) > max_length else seq + [0] * (max_length - len(seq))\n",
    "\n",
    "def get_batches(lines, batch_size):\n",
    "    random.shuffle(lines)\n",
    "    #for l in lines:\n",
    "    #    print(l[1])\n",
    "    for i in range(0, len(lines), batch_size):\n",
    "        batch = lines[i:i+batch_size]\n",
    "        if len(batch) < batch_size:\n",
    "            continue\n",
    "        neg_inputs = [pad_or_truncate(encode(p['negative'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
    "        pos_inputs = [pad_or_truncate(encode(p['positive'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
    "        neg_tensor = torch.tensor(neg_inputs, dtype=torch.long, device=device)\n",
    "        pos_tensor = torch.tensor(pos_inputs, dtype=torch.long, device=device)\n",
    "        yield neg_tensor, pos_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9d9eba",
   "metadata": {},
   "source": [
    "### Step 4: Load the pretrained NanoGPT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6f5f55",
   "metadata": {},
   "source": [
    "This step loads the checkpoint gpt.pt containing pretrained weights and config and reconstructs the model using GPTConfig. The loop cleans up by removing unwanted prefixes. Once the model is loaded and ready, it can be fine-tuned in Step 5 using the batched data and log probabilities defined in Step 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ceae772a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(74, 348)\n",
       "    (wpe): Embedding(256, 348)\n",
       "    (drop): Dropout(p=0.2, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=348, out_features=1044, bias=False)\n",
       "          (c_proj): Linear(in_features=348, out_features=348, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=348, out_features=1392, bias=False)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=1392, out_features=348, bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=348, out_features=74, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = torch.load(\"../sft/gpt.pt\", map_location=device)\n",
    "gptconf = GPTConfig(**ckpt['model_args'])\n",
    "gpt = GPT(gptconf)\n",
    "state_dict = ckpt['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k in list(state_dict.keys()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "gpt.load_state_dict(state_dict)\n",
    "gpt.to(device).train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feafc5a",
   "metadata": {},
   "source": [
    "### Step 5: Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd05aa24",
   "metadata": {},
   "source": [
    "This step is to generate the negative pair using the pretrained model(gpt.pt). \n",
    "\n",
    "generate_math_qns() generates the problems using random between 0 to 99. 10 'x' is also appended as a choice for either of the values to generate algebaric questions. If x is selected, a random integer between 0 to 99 is selected as the answer for the problem as well. \n",
    "\n",
    "The question is fed into gpt.pt with 0.1 temperature as we want to minize the randomness to consistently get the string 'Sorry, I do not know.'. The output is decoded and validated before it is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e26fe07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "source": [
    "def generate_math_qns():\n",
    "    a = random.choice(list(range(0,100)) + ['x'] * 10)\n",
    "    b = random.choice(range(0,100) if a == 'x' else list(range(0,100)) + ['x'] * 10)\n",
    "    op = random.choice(['+', '-', '*'])\n",
    "    if a == 'x' or b == 'x':\n",
    "        result = random.randint(0, 99)\n",
    "        return f\"{a}{op}{b}={result}, x=? \"\n",
    "    else:\n",
    "        return f\"{a}{op}{b}=? \"\n",
    "\n",
    "# complile examples from json\n",
    "example = ''\n",
    "with open(\"pos_neg_pairs.json\", \"r+\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "    for line in data:\n",
    "        example += line['positive'] + '\\n'\n",
    "    \n",
    "    # generate neg pos pair\n",
    "    for i in range(5):\n",
    "        prompt = generate_math_qns()\n",
    "        correct = False\n",
    "        while(not correct): # repeatedly generate neg till correct output generates\n",
    "            input_ids = torch.tensor([encode(prompt)], dtype=torch.long, device=device)\n",
    "            output = gpt.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=0.1,\n",
    "                top_k=200\n",
    "            )\n",
    "\n",
    "            negative = decode(output[0][0].tolist())\n",
    "            if negative.split('? ')[1] == \"Sorry, I don't know.\":\n",
    "                correct = True\n",
    "                pair = {\"negative\" : negative, \"positive\" : \"\"}\n",
    "                data.append(pair)\n",
    "    f.seek(0)\n",
    "    json.dump(data, f)\n",
    "    print('Negative pair generated')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fddad6f",
   "metadata": {},
   "source": [
    "This is where we load our positve negative problem pairs from the JSON file and format them for batching using get_batches(). \n",
    "\n",
    "For our training, we generated 400k pairs of data. However, during testing we realised that the algebaric problems was still not well-tuned. Hence, we generated 301k problems with 300:1 ratio of algebra problems to simple math problems. This is because we did not have enough time to fine-tune specific variables for each run (e.g learning rate, temperature, beta) and GPU capacity to train with more than 400k of data. \n",
    "\n",
    "In order to test and fine-tune specifically the algebaric problems, we sacrificed the weights for the simple math problems even though it might cause the model to lose accuracy for the simple math problems during training. We determined that this sacrifice was essential for us to pinpoint the issues with the algebaric problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7edf3d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 301000 pairs.\n",
      "Example:\n",
      "{'negative': \"x+3=97, x=? Sorry, I don't know.\", 'positive': 'x+3=97, x=? The answer is 94 because 97-3 equals 94.'}\n",
      "\n",
      "Encoded positive example:\n",
      "[71, 4, 15, 9, 21, 19, 5, 1, 71, 9, 10, 1, 41, 55, 52, 1, 48, 61, 66, 70, 52, 65, 1, 56, 66, 1, 21, 16, 1, 49, 52, 50, 48, 68, 66, 52, 1, 21, 19, 6, 15, 1, 52, 64, 68, 48, 59, 66, 1, 21]\n",
      "\n",
      "Negative batch shape: torch.Size([250, 64])\n",
      "Positive batch shape: torch.Size([250, 64])\n"
     ]
    }
   ],
   "source": [
    "# Load data from ./data/pos_neg_pairs.json\n",
    "import json\n",
    "\n",
    "with open(\"pos_neg_pairs_301k.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(data)} pairs.\")\n",
    "print(\"Example:\")\n",
    "print(data[0])\n",
    "\n",
    "sample = data[0]\n",
    "print(\"\\nEncoded positive example:\")\n",
    "print(encode(sample[\"positive\"])[:50]) \n",
    "\n",
    "batches = get_batches(data, batch_size=batch_size)\n",
    "\n",
    "neg_batch, pos_batch = next(batches)\n",
    "print(\"\\nNegative batch shape:\", neg_batch.shape)\n",
    "print(\"Positive batch shape:\", pos_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e5f81f",
   "metadata": {},
   "source": [
    "### Step 6: Build the optimizer and scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a5f3e5",
   "metadata": {},
   "source": [
    "The use of an optimizer is to facilitate finding the optimal weights of a model used to minimize the loss function.\n",
    "\n",
    "The use of a scheduler is to regulate the learning rate of the model during the training process. Initially, it will start with the learning rate set in step 2, it then slowly decreases over time, adapting the model to a better minimum after the fast initial convergence.\n",
    "\n",
    "During the training process, the optimizer updates the parameters while using the current learning rate. After a set number of epochs, the schedular then calculates a new, adjusted learning rate.\n",
    "\n",
    "AdamW Optimizer uses weight decay as compared to using Adam Optimizer gradient calculations. Rather than applying the weight decay to the loss function after calculation, it directly affects the parameters updating. \n",
    "\n",
    "The scheduler uses Cosine Annealing Scheduler Warm Restarts. Over several epochs, it will take the initial learning rate and decreases it along a cosine curve. Warm restarts periodically resets the learning rate and restarts the process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0c400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommend to use the AdamW optimizer \n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "optimizer = AdamW(\n",
    "    gpt.parameters(),\n",
    "    lr=base_lr,\n",
    "    betas=0.5,\n",
    "    eps=1e-8,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "scheduler = CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0=10,          # restart every 10 epochs\n",
    "    T_mult=1,        # no expansion of cycle length\n",
    "    eta_min=1e-6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b66199",
   "metadata": {},
   "source": [
    "### Step 7: Begin training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94983cf",
   "metadata": {},
   "source": [
    "This step is to start the training with the loaded data.\n",
    "\n",
    "For each step, we will clear the memory before computing the log probabilities for positive and negative. dpo_term calculates the DPO loss while superised_term maximizes the probability for positive samples hence reinforcing the correct answers. The loss is combined and the gradient is computed with a cap of magnitude at 1.0 using clip_grad_norm_. We also changed the checkpoint to be saved every epoch. We find that this method is useful in cases where we overtrained our data. With this implementation, we can backtrack to the checkpoint right before the overtraining.\n",
    "\n",
    "\n",
    "##### dpo_term\n",
    "First we find the difference between the positive (pos_logprob) and negative (neg_logprob) log probability. This measures the preference gap where positive difference shows preference towards positive sample and negative shows preference towards negative sample. We divide by the beta to double the difference to push the optimization to be more aggressive. logsigmold stablises the output and we take the average across the batches.\n",
    "\n",
    "##### clip_grad_norm_\n",
    "This function computes the global norm and if norm > 1.0 it will scale all the gradients down to prevent exploding gradients. This step is important for training as preference data can have extreme examples between the probabilities. The large probability gaps can cause large gradients. Hence without clipping the training will be unstable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1d4ebeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 1204it [20:14,  1.01s/it, loss=0.0268]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 1 complete | Avg loss: 0.0551\n",
      "💾 Saved checkpoint: ./dpo_epoch1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 1204it [09:59,  2.01it/s, loss=0.0222]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 2 complete | Avg loss: 0.0238\n",
      "💾 Saved checkpoint: ./dpo_epoch2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 1204it [11:23,  1.76it/s, loss=0.0209]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 3 complete | Avg loss: 0.0218\n",
      "💾 Saved checkpoint: ./dpo_epoch3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 1204it [13:38,  1.47it/s, loss=0.0213]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 4 complete | Avg loss: 0.0211\n",
      "💾 Saved checkpoint: ./dpo_epoch4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 1204it [11:50,  1.69it/s, loss=0.0205]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 5 complete | Avg loss: 0.0207\n",
      "💾 Saved checkpoint: ./dpo_epoch5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 1204it [10:55,  1.84it/s, loss=0.0206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 6 complete | Avg loss: 0.0204\n",
      "💾 Saved checkpoint: ./dpo_epoch6.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 1204it [10:00,  2.01it/s, loss=0.0195]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 7 complete | Avg loss: 0.0202\n",
      "💾 Saved checkpoint: ./dpo_epoch7.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 1204it [15:24,  1.30it/s, loss=0.0195]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 8 complete | Avg loss: 0.0199\n",
      "💾 Saved checkpoint: ./dpo_epoch8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 1204it [36:44,  1.83s/it, loss=0.0194]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 9 complete | Avg loss: 0.0197\n",
      "💾 Saved checkpoint: ./dpo_epoch9.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 1204it [10:52,  1.84it/s, loss=0.0198]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 10 complete | Avg loss: 0.0195\n",
      "💾 Saved checkpoint: ./dpo_epoch10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 1204it [22:00,  1.10s/it, loss=0.0198]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 11 complete | Avg loss: 0.0193\n",
      "💾 Saved checkpoint: ./dpo_epoch11.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 1204it [34:18,  1.71s/it, loss=0.0187]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 12 complete | Avg loss: 0.0191\n",
      "💾 Saved checkpoint: ./dpo_epoch12.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 946it [3:55:32, 14.94s/it, loss=0.0189] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Combined loss\u001b[39;00m\n\u001b[32m     24\u001b[39m loss = dpo_term + supervised_term\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Gradient clipping to prevent explosion\u001b[39;00m\n\u001b[32m     28\u001b[39m clip_grad_norm_(gpt.parameters(), \u001b[32m1.0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yeetmon\\Documents\\GitHub\\NanoGPT-test-v2\\SC3000\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yeetmon\\Documents\\GitHub\\NanoGPT-test-v2\\SC3000\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yeetmon\\Documents\\GitHub\\NanoGPT-test-v2\\SC3000\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    gpt.train()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(get_batches(data, batch_size), desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    for step, (neg_tensor, pos_tensor) in enumerate(pbar):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute log probabilities\n",
    "        pos_logprob = compute_logprob(pos_tensor)\n",
    "        neg_logprob = compute_logprob(neg_tensor)\n",
    "\n",
    "        # DPO loss (main preference objective)\n",
    "        dpo_term = -F.logsigmoid((pos_logprob - neg_logprob) / beta).mean()\n",
    "\n",
    "        # Auxiliary supervised loss — reinforces correct answers\n",
    "        supervised_term = -pos_logprob.mean() * 0.1\n",
    "\n",
    "        # Combined loss\n",
    "        loss = dpo_term + supervised_term\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping to prevent explosion\n",
    "        clip_grad_norm_(gpt.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        train_losses.append(loss.item())\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / (step + 1)\n",
    "    print(f\"✅ Epoch {epoch+1} complete | Avg loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Save checkpoints every epoch\n",
    "    ckpt_path = f\"./dpo_epoch{epoch+1}.pt\"\n",
    "    torch.save({\n",
    "        \"model_state_dict\": gpt.state_dict(),\n",
    "        \"model_args\": ckpt['model_args']\n",
    "    }, ckpt_path)\n",
    "    print(f\"💾 Saved checkpoint: {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b7f2ab",
   "metadata": {},
   "source": [
    "### Step 8: Begin testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002b5466",
   "metadata": {},
   "source": [
    "Loads the trained model after the training process. We saved each epoch to be able to trace back to whichever epoch we want to use for testing, checking for overtraining. This allows us to select and examine our best performance on the training data.\n",
    "\n",
    "In addition to the given test set, we added more test sets for improved reliability. \n",
    "\n",
    "The test function iterates through all the prompts in the test set and encodes them from string to token IDs and runs it through the correct device.\n",
    "\n",
    "It then generates the response based on the given parameters. max_new_tokens which sets the max generation length to 200, allows for sufficient full explanation of response. top_k which filters to the top 200 tokens, prevents unlikely tokens from being sampled. As for the temperature we played around with different temperatures varying from 0.1 (more deterministic) to 0.9 (more diverse), checking which gives the best performance.\n",
    "\n",
    "The decoder extracts the tensors and places them into a python list, while the token IDs are converted back into strings. Finally, displaying the responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09027262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 17+19=?\n",
      "Model Output: 17+19=? The answer is 188 because 17+191 equals 18.\n",
      "\n",
      "Prompt: 3*17=?\n",
      "Model Output: 3*17=? x=? The answer is 1 because 3/1 equals 1.\n",
      "\n",
      "Prompt: 72/4=?\n",
      "Model Output: 72/4=? x=? The answer is 8 because 72/1 equals 88.\n",
      "\n",
      "Prompt: 72-x=34,x=?\n",
      "Model Output: 72-x=34,x=? The answer is 69 because 72-3 equals 69.\n",
      "\n",
      "Prompt: x*11=44,x=?\n",
      "Model Output: x*11=44,x=? The answer is 44 because 44/1 equals 44.\n",
      "\n",
      "Prompt: 3*17=?\n",
      "Model Output: 3*17=? x=? The answer is 1 because 3/1 equals 1.\n",
      "\n",
      "Prompt: 72/4=?\n",
      "Model Output: 72/4=? x=? The answer is 8 because 72/1 equals 88.\n",
      "\n",
      "Prompt: 72-x=34,x=?\n",
      "Model Output: 72-x=34,x=? The answer is 69 because 72-3 equals 69.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model\n",
    "ckpt_path = \"../dpo/dpo_epoch12.pt\"\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "gpt = GPT(gptconf).to(device)\n",
    "try:\n",
    "    state_dict = checkpoint['model']\n",
    "except:\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "gpt.load_state_dict(state_dict)\n",
    "# Test\n",
    "gpt.eval()\n",
    "test_set = [\"17+19=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\", \"x*11=44,x=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\"]\n",
    "# test_set = [\n",
    "#     \"25+18=?\",\n",
    "#     \"56-27=?\",\n",
    "#     \"9*12=?\",\n",
    "#     \"84/7=?\",\n",
    "#     \"x+45=90,x=?\",\n",
    "#     \"63-x=22,x=?\",\n",
    "#     \"x*6=54,x=?\",\n",
    "#     \"88/11=?\"\n",
    "# ]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for prompt in test_set: \n",
    "        # prompt_ids = encode(prompt)\n",
    "        prompt_ids = torch.tensor([encode(prompt)], dtype=torch.long, device=device)\n",
    "        output = gpt.generate(\n",
    "            prompt_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.01,\n",
    "            top_k=200\n",
    "        )\n",
    "        result = decode(output[0][0].tolist())\n",
    "        print(f\"Prompt: {prompt}\\nModel Output: {result}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SC3000",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
